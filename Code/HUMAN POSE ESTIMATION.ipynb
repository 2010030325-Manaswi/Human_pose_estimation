{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06414696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\rudra\\anaconda3\\lib\\site-packages (4.7.0.68)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\rudra\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fc30238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in c:\\users\\rudra\\anaconda3\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\rudra\\anaconda3\\lib\\site-packages (from tensorflow_hub) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\rudra\\anaconda3\\lib\\site-packages (from tensorflow_hub) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b662c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer vision/graphics library\n",
    "import cv2\n",
    "\n",
    "# Gif writer\n",
    "import imageio \n",
    "\n",
    "# Display libraries \n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Calculations and Deep Learning library\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699b902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99ed9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cyan = (255, 255, 0)\n",
    "magenta = (255, 0, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d24738d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_COLORS = {\n",
    "    (0, 1): magenta,\n",
    "    (0, 2): cyan,\n",
    "    (1, 3): magenta,\n",
    "    (2, 4): cyan,\n",
    "    (0, 5): magenta,\n",
    "    (0, 6): cyan,\n",
    "    (5, 7): magenta,\n",
    "    (7, 9): cyan,\n",
    "    (6, 8): magenta,\n",
    "    (8, 10): cyan,\n",
    "    (5, 6): magenta,\n",
    "    (5, 11): cyan,\n",
    "    (6, 12): magenta,\n",
    "    (11, 12): cyan,\n",
    "    (11, 13): magenta,\n",
    "    (13, 15): cyan,\n",
    "    (12, 14): magenta,\n",
    "    (14, 16): cyan\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(\"https://tfhub.dev/google/movenet/multipose/lightning/1\")\n",
    "movenet = model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e74bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O ngannou.gif https://raw.githubusercontent.com/Justsecret123/Human-pose-estimation/main/Test%20gifs/Ngannou_3.gif --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(frame, keypoints, threshold=0.11):\n",
    "    \"\"\"\n",
    "    Main loop : Draws the keypoints and edges for each instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loop through the results\n",
    "    for instance in keypoints: \n",
    "        # Draw the keypoints and get the denormalized coordinates\n",
    "        denormalized_coordinates = draw_keypoints(frame, instance, threshold)\n",
    "        # Draw the edges\n",
    "        draw_edges(denormalized_coordinates, frame, EDGE_COLORS, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdf9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_width, initial_height = (461,250)\n",
    "WIDTH = HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7319c37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, threshold=0.11):\n",
    "    \"\"\"Draws the keypoints on a image frame\"\"\"\n",
    "    \n",
    "    # Denormalize the coordinates : multiply the normalized coordinates by the input_size(width,height)\n",
    "    denormalized_coordinates = np.squeeze(np.multiply(keypoints, [WIDTH,HEIGHT,1]))\n",
    "    #Iterate through the points\n",
    "    for keypoint in denormalized_coordinates:\n",
    "        # Unpack the keypoint values : y, x, confidence score\n",
    "        keypoint_y, keypoint_x, keypoint_confidence = keypoint\n",
    "        if keypoint_confidence > threshold:\n",
    "            \"\"\"\"\n",
    "            Draw the circle\n",
    "            Note : A thickness of -1 px will fill the circle shape by the specified color.\n",
    "            \"\"\"\n",
    "            cv2.circle(\n",
    "                img=frame, \n",
    "                center=(int(keypoint_x), int(keypoint_y)), \n",
    "                radius=4, \n",
    "                color=(255,0,0),\n",
    "                thickness=-1\n",
    "            )\n",
    "    return denormalized_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6243038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_edges(denormalized_coordinates, frame, edges_colors, threshold=0.11):\n",
    "    \"\"\"\n",
    "    Draws the edges on a image frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate through the edges \n",
    "    for edge, color in edges_colors.items():\n",
    "        # Get the dict value associated to the actual edge\n",
    "        p1, p2 = edge\n",
    "        # Get the points\n",
    "        y1, x1, confidence_1 = denormalized_coordinates[p1]\n",
    "        y2, x2, confidence_2 = denormalized_coordinates[p2]\n",
    "        # Draw the line from point 1 to point 2, the confidence > threshold\n",
    "        if (confidence_1 > threshold) & (confidence_2 > threshold):      \n",
    "            cv2.line(\n",
    "                img=frame, \n",
    "                pt1=(int(x1), int(y1)),\n",
    "                pt2=(int(x2), int(y2)), \n",
    "                color=color, \n",
    "                thickness=2, \n",
    "                lineType=cv2.LINE_AA # Gives anti-aliased (smoothed) line which looks great for curves\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress(value, max=100):\n",
    "    \"\"\"\n",
    "    Returns an HTML progress bar with a certain value. Used within each step\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value,\n",
    "                max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gif():\n",
    "    \"\"\"\n",
    "    Loads the gif and return its details\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the gif\n",
    "    gif = cv2.VideoCapture(\"./h3.mp4\")\n",
    "    # Get the frame count\n",
    "    frame_count = int(gif.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    # Display parameter\n",
    "    print(f\"Frame count: {frame_count}\")\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    Initialize the video writer \n",
    "    We'll append each frame and its drawing to a vector, then stack all the frames to obtain a sequence (video). \n",
    "    \"\"\"\n",
    "    output_frames = []\n",
    "    \n",
    "    # Get the initial shape (width, height)\n",
    "    initial_shape = []\n",
    "    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_WIDTH)))\n",
    "    initial_shape.append(int(gif.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    \n",
    "    return gif, frame_count, output_frames, initial_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference():\n",
    "    \"\"\"\n",
    "    Runs inferences then starts the main loop for each frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the gif\n",
    "    gif, frame_count, output_frames, initial_shape = load_gif()\n",
    "    # Set the progress bar to 0. It ranges from the first to the last frame\n",
    "    bar = display(progress(0, frame_count-1), display_id=True)\n",
    "    \n",
    "    # Loop while the gif is opened\n",
    "    while gif.isOpened():\n",
    "        \n",
    "        # Capture the frame\n",
    "        ret, frame = gif.read()\n",
    "        \n",
    "        # Exit if the frame is empty\n",
    "        if frame is None: \n",
    "            break\n",
    "        \n",
    "        # Retrieve the frame index\n",
    "        current_index = gif.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        \n",
    "        # Copy the frame\n",
    "        image = frame.copy()\n",
    "        image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "        # Resize to the target shape and cast to an int32 vector\n",
    "        input_image = tf.cast(tf.image.resize_with_pad(image, WIDTH, HEIGHT), dtype=tf.int32)\n",
    "        # Create a batch (input tensor)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        # Perform inference\n",
    "        results = movenet(input_image)\n",
    "        \"\"\"\n",
    "        Output shape :  [1, 6, 56] ---> (batch size), (instances), (xy keypoints coordinates and score from [0:50] \n",
    "        and [ymin, xmin, ymax, xmax, score] for the remaining elements)\n",
    "        First, let's resize it to a more convenient shape, following this logic : \n",
    "        - First channel ---> each instance\n",
    "        - Second channel ---> 17 keypoints for each instance\n",
    "        - The 51st values of the last channel ----> the confidence score.\n",
    "        Thus, the Tensor is reshaped without losing important information. \n",
    "        \"\"\"\n",
    "        \n",
    "        keypoints = results[\"output_0\"].numpy()[:,:,:51].reshape((6,17,3))\n",
    "\n",
    "        # Loop through the results\n",
    "        loop(image, keypoints, threshold=0.11)\n",
    "        \n",
    "        # Get the output frame : reshape to the original size\n",
    "        frame_rgb = cv2.cvtColor(\n",
    "            cv2.resize(\n",
    "                image,(initial_shape[0], initial_shape[1]), \n",
    "                interpolation=cv2.INTER_LANCZOS4\n",
    "            ), \n",
    "            cv2.COLOR_BGR2RGB # OpenCV processes BGR images instead of RGB\n",
    "        ) \n",
    "        \n",
    "        # Add the drawings to the output frames\n",
    "        output_frames.append(frame_rgb)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        bar.update(progress(current_index, frame_count-1))\n",
    "    \n",
    "    # Release the object\n",
    "    gif.release()\n",
    "    \n",
    "    print(\"Completed !\")\n",
    "    \n",
    "    return output_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf81a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frames = run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow_docs.vis import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71766c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.stack(output_frames, axis=0) \n",
    "# Write the sequence to a gif\n",
    "imageio.mimsave(\"./animation.gif\", output, fps=15) \n",
    "# Embed the output to the notebook\n",
    "embed.embed_file(\"./animation.gif\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360f5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
